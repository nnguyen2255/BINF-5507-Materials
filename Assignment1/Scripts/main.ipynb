{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before imputation, missing values in the dataset: 4588\n",
      "After imputation, missing values in the dataset: 0\n",
      "After impute: <class 'pandas.core.frame.DataFrame'>\n",
      "--------------------\n",
      "Before removing duplicates, duplicate rows in the dataset: 0\n",
      "Before removing duplicates, number of rows and columns in the dataset: (1196, 28)\n",
      "After removing duplicates, duplicate rows in the dataset: 0\n",
      "After removing duplicates, number of rows and columns in the dataset: (1196, 28)\n",
      "After remove duplicates: <class 'pandas.core.frame.DataFrame'>\n",
      "--------------------\n",
      "After normalize: <class 'pandas.core.frame.DataFrame'>\n",
      "--------------------\n",
      "Before removing redundant features: (1196, 28)\n",
      "After removing redundant features: (1196, 22)\n",
      "At threshold =  0.9 Removed repundant features: ['o', 'p', 'q', 'r', 'u', 'y']\n",
      "--------------------\n",
      "Cleaned data saved successfully.\n",
      "--------------------\n",
      "Accuracy: 0.7333333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77       138\n",
      "           1       0.68      0.70      0.69       102\n",
      "\n",
      "    accuracy                           0.73       240\n",
      "   macro avg       0.73      0.73      0.73       240\n",
      "weighted avg       0.73      0.73      0.73       240\n",
      "\n",
      "Read more about the classification report: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html and https://www.nb-data.com/p/breaking-down-the-classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhinguyen/Desktop/HUMBER/BINF5507-Git/BINF-5507-Materials/BINF-5507-Materials-3/Assignment1/Scripts/data_preprocessor.py:27: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_copy[col]= data_copy[col].fillna(data_copy[col].mode()[0])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import data_preprocessor as dp\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import importlib #to reload the module if needed\n",
    "importlib.reload(dp)\n",
    "# Reload the data_preprocessor module to ensure any changes are reflected\n",
    "#I used ChatGPT to help with error structure and troubleshooting (ChatGPT, 2025)\n",
    "\n",
    "# 1. Load the dataset\n",
    "messy_data = pd.read_csv('../Data/messy_data.csv')\n",
    "clean_data = messy_data.copy()\n",
    "\n",
    "\n",
    "# #Display dataset information\n",
    "# messy_data.head()\n",
    "# messy_data.info()\n",
    "# messy_data.describe()\n",
    "\n",
    "\n",
    "#_________________________________\n",
    "\n",
    "\n",
    "# # 2. Preprocess the data\n",
    "\n",
    "#IMPUTE MISSING VALUES\n",
    "clean_data = dp.impute_missing_values(clean_data, strategy='mean')\n",
    "print(\"After impute:\", type(clean_data)) #to check and make sure the data is still a DataFrame\n",
    "\n",
    "print(\"-\"*20) #to separate outputs\n",
    "\n",
    "\n",
    "#REMOVE DUPLICATES\n",
    "#check number of (row,columns) before/after removing duplicates\n",
    "clean_data = dp.remove_duplicates(clean_data)\n",
    "print(\"After remove duplicates:\", type(clean_data))\n",
    "\n",
    "print(\"-\"*20) #to separate outputs\n",
    "\n",
    "\n",
    "\n",
    "#NORMALIZE DATA\n",
    "clean_data = dp.normalize_data(clean_data, method='minmax')\n",
    "print(\"After normalize:\", type(clean_data))\n",
    "\n",
    "print(\"-\"*20) #to separate outputs\n",
    "\n",
    "\n",
    "#REMOVING REDUNDANT FEATURES\n",
    "#check number of (rows,columns) before/after removing redundant features\n",
    "clean_data = dp.remove_redundant_features(clean_data)\n",
    "\n",
    "print(\"-\"*20) #to separate outputs\n",
    "\n",
    "\n",
    "\n",
    "# # 3. SAVE THE CLEANED DATA\n",
    "clean_data.to_csv('../Data/clean_data.csv', index=False)\n",
    "if clean_data.empty:\n",
    "    print(\"The cleaned data is empty.\")\n",
    "else:\n",
    "    print(\"Cleaned data saved successfully.\")\n",
    "    \n",
    "print(\"-\"*20) #to separate outputs    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 4. TRAIN A SIMPLE MODEL\n",
    "#ValueError: Unknown label type: continuous. (ChatGPT, 2025)-> \n",
    "#trying to fit a classifier, which expects discrete classes on a regression target with continuous values.\n",
    "#target variable (y) should be categorical, so we will convert 'target'(y) to integer instead of float\n",
    "#ensure the target variable is categorical\n",
    "clean_data['target']=clean_data['target'].astype(int) #convert target to int\n",
    "dp.simple_model(clean_data, print_report= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
